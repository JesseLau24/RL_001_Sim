{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e04264af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "import numpy as np\n",
    "\n",
    "class SpeculationVsLaborEnv(gym.Env):\n",
    "    def __init__(self,\n",
    "                 max_age=365*80,\n",
    "                 labor_income=100,\n",
    "                 labor_health_cost=2,\n",
    "                 p_win=0.4,\n",
    "                 speculative_win_return=500,\n",
    "                 speculative_loss=400,\n",
    "                 invest_health_cost=1,\n",
    "                 rest_health_gain=3,\n",
    "                 r_save=0.0001,   # daily saving interest\n",
    "                 r_debt=0.0005):  # daily debt interest\n",
    "        super(SpeculationVsLaborEnv, self).__init__()\n",
    "\n",
    "        # --- Parameters ---\n",
    "        self.max_age = max_age\n",
    "        self.labor_income = labor_income\n",
    "        self.labor_health_cost = labor_health_cost\n",
    "        self.p_win = p_win\n",
    "        self.speculative_win_return = speculative_win_return\n",
    "        self.speculative_loss = speculative_loss\n",
    "        self.invest_health_cost = invest_health_cost\n",
    "        self.rest_health_gain = rest_health_gain\n",
    "        self.r_save = r_save\n",
    "        self.r_debt = r_debt\n",
    "\n",
    "        # --- Action space: 0 = Labor, 1 = Speculate, 2 = Rest ---\n",
    "        self.action_space = spaces.Discrete(3)\n",
    "\n",
    "        # --- Observation space: [wealth, health, age, debt, savings] ---\n",
    "        high = np.array([1e9, 100, max_age, 1e9, 1e9], dtype=np.float32)\n",
    "        low  = np.array([-1e9, 0, 0, 0, 0], dtype=np.float32)\n",
    "        self.observation_space = spaces.Box(low, high, dtype=np.float32)\n",
    "\n",
    "        # Initialize state\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        super().reset(seed=seed)\n",
    "        self.wealth = 0.0\n",
    "        self.health = 100.0\n",
    "        self.age = 0\n",
    "        self.debt = 0.0\n",
    "        self.savings = 0.0\n",
    "\n",
    "        # --- Reset action counters ---\n",
    "        self.work_count = 0\n",
    "        self.speculate_count = 0\n",
    "        self.rest_count = 0\n",
    "\n",
    "        obs = self._get_obs()\n",
    "        return obs\n",
    "\n",
    "    def _get_obs(self):\n",
    "        return np.array([self.wealth, self.health, self.age, self.debt, self.savings], dtype=np.float32)\n",
    "\n",
    "    def step(self, action):\n",
    "        reward = 0\n",
    "\n",
    "        # --- Take action ---\n",
    "        if action == 0:  # Labor\n",
    "            self.wealth += self.labor_income\n",
    "            self.health -= self.labor_health_cost\n",
    "            self.work_count += 1\n",
    "\n",
    "        elif action == 1:  # Speculate\n",
    "            if np.random.rand() < self.p_win:\n",
    "                self.wealth += self.speculative_win_return\n",
    "            else:\n",
    "                self.wealth -= self.speculative_loss\n",
    "            self.health -= self.invest_health_cost\n",
    "            self.speculate_count += 1\n",
    "\n",
    "        elif action == 2:  # Rest\n",
    "            self.health = min(100, self.health + self.rest_health_gain)\n",
    "            self.rest_count += 1\n",
    "\n",
    "        # --- Daily financial updates ---\n",
    "        if self.wealth >= 0:\n",
    "            self.savings = self.wealth\n",
    "            self.savings *= (1 + self.r_save)\n",
    "            self.wealth = self.savings\n",
    "        else:\n",
    "            self.debt = -self.wealth\n",
    "            self.debt *= (1 + self.r_debt)\n",
    "            self.wealth = -self.debt\n",
    "\n",
    "        # --- Advance time ---\n",
    "        self.age += 1\n",
    "\n",
    "        # --- Reward: use current wealth (you could also use delta) ---\n",
    "        reward = self.wealth\n",
    "\n",
    "        # --- Termination conditions ---\n",
    "        terminated = self.age >= self.max_age or self.health <= 0 or self.wealth < -1e6\n",
    "\n",
    "        obs = self._get_obs()\n",
    "\n",
    "        # --- Info dictionary with counters ---\n",
    "        info = {\n",
    "            \"work_count\": self.work_count,\n",
    "            \"speculate_count\": self.speculate_count,\n",
    "            \"rest_count\": self.rest_count\n",
    "        }\n",
    "\n",
    "        # --- Episode stats (only when terminated) ---\n",
    "        if terminated:\n",
    "            info[\"episode\"] = {\n",
    "                \"r\": self.wealth,                    # 这里是最终财富\n",
    "                \"l\": self.age,                       # 存活天数\n",
    "                \"days_worked\": self.work_count,\n",
    "                \"days_speculated\": self.speculate_count,\n",
    "                \"days_rested\": self.rest_count,\n",
    "                \"final_age\": self.age / 365,\n",
    "                \"final_wealth\": self.wealth\n",
    "            }\n",
    "\n",
    "        return obs, reward, terminated, False, info\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b2482e76",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "\n",
    "class EpisodeLoggerCallback(BaseCallback):\n",
    "    def __init__(self, verbose=1):\n",
    "        super(EpisodeLoggerCallback, self).__init__(verbose)\n",
    "        self.episode_logs = []   # 存放每个 episode 的结果\n",
    "\n",
    "    def _on_step(self) -> bool:\n",
    "        # infos 是字典 list（每个并行环境一个）\n",
    "        for info in self.locals[\"infos\"]:\n",
    "            if \"episode\" in info.keys():\n",
    "                ep = info[\"episode\"]\n",
    "                record = {\n",
    "                    \"r\": ep.get(\"r\", 0),                 # 总回报（这里是 final wealth）\n",
    "                    \"l\": ep.get(\"l\", 0),                 # 存活时长（天数）\n",
    "                    \"t\": self.num_timesteps,             # 全局步数\n",
    "                    \"days_worked\": ep.get(\"days_worked\", 0),\n",
    "                    \"days_speculated\": ep.get(\"days_speculated\", 0),\n",
    "                    \"days_rested\": ep.get(\"days_rested\", 0),\n",
    "                    \"final_age\": ep.get(\"final_age\", 0),\n",
    "                    \"final_wealth\": ep.get(\"final_wealth\", 0)\n",
    "                }\n",
    "                self.episode_logs.append(record)\n",
    "\n",
    "                if self.verbose > 0:\n",
    "                    print(f\"[EpisodeLogger] steps={record['l']} \"\n",
    "                          f\"return={record['r']:.2f} \"\n",
    "                          f\"worked={record['days_worked']} \"\n",
    "                          f\"speculated={record['days_speculated']} \"\n",
    "                          f\"rested={record['days_rested']} \"\n",
    "                          f\"age={record['final_age']:.2f} \"\n",
    "                          f\"wealth={record['final_wealth']:.2f} \"\n",
    "                          f\"total_steps={record['t']}\")\n",
    "        return True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "40ac56c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "开始训练，总步数：43800\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 1196 |\n",
      "|    iterations      | 1    |\n",
      "|    time_elapsed    | 0    |\n",
      "|    total_timesteps | 1024 |\n",
      "-----------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 705           |\n",
      "|    iterations           | 2             |\n",
      "|    time_elapsed         | 2             |\n",
      "|    total_timesteps      | 2048          |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.1234079e-08 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.1          |\n",
      "|    explained_variance   | -5.96e-07     |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 4.1e+10       |\n",
      "|    n_updates            | 10            |\n",
      "|    policy_gradient_loss | 1.02e-07      |\n",
      "|    value_loss           | 1.59e+11      |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 815          |\n",
      "|    iterations           | 3            |\n",
      "|    time_elapsed         | 3            |\n",
      "|    total_timesteps      | 3072         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 6.170012e-09 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.1         |\n",
      "|    explained_variance   | 2.38e-07     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.97e+11     |\n",
      "|    n_updates            | 20           |\n",
      "|    policy_gradient_loss | -2.06e-07    |\n",
      "|    value_loss           | 6.56e+11     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 884          |\n",
      "|    iterations           | 4            |\n",
      "|    time_elapsed         | 4            |\n",
      "|    total_timesteps      | 4096         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 7.508788e-09 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.1         |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 9.81e+11     |\n",
      "|    n_updates            | 30           |\n",
      "|    policy_gradient_loss | -7.12e-07    |\n",
      "|    value_loss           | 2.11e+12     |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 933           |\n",
      "|    iterations           | 5             |\n",
      "|    time_elapsed         | 5             |\n",
      "|    total_timesteps      | 5120          |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | -9.895302e-10 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.1          |\n",
      "|    explained_variance   | 1.19e-07      |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 2.14e+12      |\n",
      "|    n_updates            | 40            |\n",
      "|    policy_gradient_loss | 1.76e-07      |\n",
      "|    value_loss           | 3.89e+12      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 968           |\n",
      "|    iterations           | 6             |\n",
      "|    time_elapsed         | 6             |\n",
      "|    total_timesteps      | 6144          |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 8.8475645e-09 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.1          |\n",
      "|    explained_variance   | 0             |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 3.81e+12      |\n",
      "|    n_updates            | 50            |\n",
      "|    policy_gradient_loss | 3.49e-07      |\n",
      "|    value_loss           | 6.52e+12      |\n",
      "-------------------------------------------\n",
      "[EpisodeLogger] steps=6523 return=611980160.00 worked=0 speculated=0 rested=0 age=0.00 wealth=0.00 total_steps=6523\n",
      "[EpisodeLogger] steps=463 return=2487373.50 worked=0 speculated=0 rested=0 age=0.00 wealth=0.00 total_steps=6986\n",
      "--------------------------------------------\n",
      "| rollout/                |                |\n",
      "|    ep_len_mean          | 3.49e+03       |\n",
      "|    ep_rew_mean          | 3.07e+08       |\n",
      "| time/                   |                |\n",
      "|    fps                  | 995            |\n",
      "|    iterations           | 7              |\n",
      "|    time_elapsed         | 7              |\n",
      "|    total_timesteps      | 7168           |\n",
      "| train/                  |                |\n",
      "|    approx_kl            | -5.5879354e-09 |\n",
      "|    clip_fraction        | 0              |\n",
      "|    clip_range           | 0.2            |\n",
      "|    entropy_loss         | -1.1           |\n",
      "|    explained_variance   | 0              |\n",
      "|    learning_rate        | 0.0003         |\n",
      "|    loss                 | 4.22e+12       |\n",
      "|    n_updates            | 60             |\n",
      "|    policy_gradient_loss | 8.81e-07       |\n",
      "|    value_loss           | 9.94e+12       |\n",
      "--------------------------------------------\n",
      "[EpisodeLogger] steps=828 return=7864898.50 worked=0 speculated=0 rested=0 age=0.00 wealth=0.00 total_steps=7814\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 2.6e+03       |\n",
      "|    ep_rew_mean          | 2.07e+08      |\n",
      "| time/                   |               |\n",
      "|    fps                  | 798           |\n",
      "|    iterations           | 8             |\n",
      "|    time_elapsed         | 10            |\n",
      "|    total_timesteps      | 8192          |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 2.5087502e-08 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.1          |\n",
      "|    explained_variance   | 5.96e-07      |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 1.81e+12      |\n",
      "|    n_updates            | 70            |\n",
      "|    policy_gradient_loss | -2.29e-06     |\n",
      "|    value_loss           | 4.44e+12      |\n",
      "-------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 2.6e+03     |\n",
      "|    ep_rew_mean          | 2.07e+08    |\n",
      "| time/                   |             |\n",
      "|    fps                  | 747         |\n",
      "|    iterations           | 9           |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 9216        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 8.20728e-09 |\n",
      "|    clip_fraction        | 0           |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.1        |\n",
      "|    explained_variance   | 3.1e-06     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 3.34e+10    |\n",
      "|    n_updates            | 80          |\n",
      "|    policy_gradient_loss | -6.51e-07   |\n",
      "|    value_loss           | 4.56e+10    |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 2.6e+03      |\n",
      "|    ep_rew_mean          | 2.07e+08     |\n",
      "| time/                   |              |\n",
      "|    fps                  | 777          |\n",
      "|    iterations           | 10           |\n",
      "|    time_elapsed         | 13           |\n",
      "|    total_timesteps      | 10240        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 1.169974e-08 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.1         |\n",
      "|    explained_variance   | 2.98e-07     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.62e+11     |\n",
      "|    n_updates            | 90           |\n",
      "|    policy_gradient_loss | -2.48e-07    |\n",
      "|    value_loss           | 2.88e+11     |\n",
      "------------------------------------------\n",
      "[EpisodeLogger] steps=3056 return=130537424.00 worked=0 speculated=0 rested=0 age=0.00 wealth=0.00 total_steps=10870\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 2.72e+03      |\n",
      "|    ep_rew_mean          | 1.88e+08      |\n",
      "| time/                   |               |\n",
      "|    fps                  | 802           |\n",
      "|    iterations           | 11            |\n",
      "|    time_elapsed         | 14            |\n",
      "|    total_timesteps      | 11264         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 8.1490725e-09 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.1          |\n",
      "|    explained_variance   | 0             |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 3.77e+11      |\n",
      "|    n_updates            | 100           |\n",
      "|    policy_gradient_loss | -6.54e-07     |\n",
      "|    value_loss           | 1.07e+12      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 2.72e+03      |\n",
      "|    ep_rew_mean          | 1.88e+08      |\n",
      "| time/                   |               |\n",
      "|    fps                  | 770           |\n",
      "|    iterations           | 12            |\n",
      "|    time_elapsed         | 15            |\n",
      "|    total_timesteps      | 12288         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.1292286e-08 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.1          |\n",
      "|    explained_variance   | 0             |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 8.68e+09      |\n",
      "|    n_updates            | 110           |\n",
      "|    policy_gradient_loss | 2.27e-07      |\n",
      "|    value_loss           | 1.22e+12      |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 2.72e+03     |\n",
      "|    ep_rew_mean          | 1.88e+08     |\n",
      "| time/                   |              |\n",
      "|    fps                  | 792          |\n",
      "|    iterations           | 13           |\n",
      "|    time_elapsed         | 16           |\n",
      "|    total_timesteps      | 13312        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 1.740409e-08 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.1         |\n",
      "|    explained_variance   | -1.19e-07    |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 8.05e+10     |\n",
      "|    n_updates            | 120          |\n",
      "|    policy_gradient_loss | -2.42e-07    |\n",
      "|    value_loss           | 2.65e+11     |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 2.72e+03    |\n",
      "|    ep_rew_mean          | 1.88e+08    |\n",
      "| time/                   |             |\n",
      "|    fps                  | 811         |\n",
      "|    iterations           | 14          |\n",
      "|    time_elapsed         | 17          |\n",
      "|    total_timesteps      | 14336       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 5.47152e-09 |\n",
      "|    clip_fraction        | 0           |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.1        |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 6.07e+11    |\n",
      "|    n_updates            | 130         |\n",
      "|    policy_gradient_loss | -5.71e-08   |\n",
      "|    value_loss           | 1.02e+12    |\n",
      "-----------------------------------------\n",
      "[EpisodeLogger] steps=3702 return=179679008.00 worked=0 speculated=0 rested=0 age=0.00 wealth=0.00 total_steps=14572\n",
      "[EpisodeLogger] steps=719 return=6967857.00 worked=0 speculated=0 rested=0 age=0.00 wealth=0.00 total_steps=15291\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 2.55e+03      |\n",
      "|    ep_rew_mean          | 1.57e+08      |\n",
      "| time/                   |               |\n",
      "|    fps                  | 829           |\n",
      "|    iterations           | 15            |\n",
      "|    time_elapsed         | 18            |\n",
      "|    total_timesteps      | 15360         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 5.2386895e-09 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.1          |\n",
      "|    explained_variance   | -1.19e-07     |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 9.77e+11      |\n",
      "|    n_updates            | 140           |\n",
      "|    policy_gradient_loss | -7.3e-07      |\n",
      "|    value_loss           | 2.04e+12      |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 2.55e+03     |\n",
      "|    ep_rew_mean          | 1.57e+08     |\n",
      "| time/                   |              |\n",
      "|    fps                  | 766          |\n",
      "|    iterations           | 16           |\n",
      "|    time_elapsed         | 21           |\n",
      "|    total_timesteps      | 16384        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 4.947651e-09 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.1         |\n",
      "|    explained_variance   | 1.19e-06     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.66e+10     |\n",
      "|    n_updates            | 150          |\n",
      "|    policy_gradient_loss | -5.87e-07    |\n",
      "|    value_loss           | 7.93e+11     |\n",
      "------------------------------------------\n",
      "[EpisodeLogger] steps=2116 return=42622548.00 worked=0 speculated=0 rested=0 age=0.00 wealth=0.00 total_steps=17407\n",
      "--------------------------------------------\n",
      "| rollout/                |                |\n",
      "|    ep_len_mean          | 2.49e+03       |\n",
      "|    ep_rew_mean          | 1.4e+08        |\n",
      "| time/                   |                |\n",
      "|    fps                  | 782            |\n",
      "|    iterations           | 17             |\n",
      "|    time_elapsed         | 22             |\n",
      "|    total_timesteps      | 17408          |\n",
      "| train/                  |                |\n",
      "|    approx_kl            | 1.22818165e-08 |\n",
      "|    clip_fraction        | 0              |\n",
      "|    clip_range           | 0.2            |\n",
      "|    entropy_loss         | -1.1           |\n",
      "|    explained_variance   | 7.65e-05       |\n",
      "|    learning_rate        | 0.0003         |\n",
      "|    loss                 | 7.02e+09       |\n",
      "|    n_updates            | 160            |\n",
      "|    policy_gradient_loss | -8.65e-07      |\n",
      "|    value_loss           | 3.8e+10        |\n",
      "--------------------------------------------\n",
      "[EpisodeLogger] steps=932 return=4104466.75 worked=0 speculated=0 rested=0 age=0.00 wealth=0.00 total_steps=18339\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 2.29e+03      |\n",
      "|    ep_rew_mean          | 1.23e+08      |\n",
      "| time/                   |               |\n",
      "|    fps                  | 763           |\n",
      "|    iterations           | 18            |\n",
      "|    time_elapsed         | 24            |\n",
      "|    total_timesteps      | 18432         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 8.1490725e-09 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.1          |\n",
      "|    explained_variance   | 1.79e-07      |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 1.41e+11      |\n",
      "|    n_updates            | 170           |\n",
      "|    policy_gradient_loss | 2.44e-07      |\n",
      "|    value_loss           | 4.5e+11       |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 2.29e+03     |\n",
      "|    ep_rew_mean          | 1.23e+08     |\n",
      "| time/                   |              |\n",
      "|    fps                  | 750          |\n",
      "|    iterations           | 19           |\n",
      "|    time_elapsed         | 25           |\n",
      "|    total_timesteps      | 19456        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 7.043127e-09 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.1         |\n",
      "|    explained_variance   | 8.69e-05     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 4.53e+09     |\n",
      "|    n_updates            | 180          |\n",
      "|    policy_gradient_loss | -4.6e-07     |\n",
      "|    value_loss           | 1.28e+10     |\n",
      "------------------------------------------\n",
      "[EpisodeLogger] steps=2020 return=42320568.00 worked=0 speculated=0 rested=0 age=0.00 wealth=0.00 total_steps=20359\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 2.26e+03      |\n",
      "|    ep_rew_mean          | 1.14e+08      |\n",
      "| time/                   |               |\n",
      "|    fps                  | 764           |\n",
      "|    iterations           | 20            |\n",
      "|    time_elapsed         | 26            |\n",
      "|    total_timesteps      | 20480         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.5599653e-08 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.1          |\n",
      "|    explained_variance   | -1.19e-07     |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 4.46e+10      |\n",
      "|    n_updates            | 190           |\n",
      "|    policy_gradient_loss | -2.19e-06     |\n",
      "|    value_loss           | 6.71e+10      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 2.26e+03      |\n",
      "|    ep_rew_mean          | 1.14e+08      |\n",
      "| time/                   |               |\n",
      "|    fps                  | 752           |\n",
      "|    iterations           | 21            |\n",
      "|    time_elapsed         | 28            |\n",
      "|    total_timesteps      | 21504         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.2165401e-08 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.1          |\n",
      "|    explained_variance   | 9.54e-07      |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 4.46e+10      |\n",
      "|    n_updates            | 200           |\n",
      "|    policy_gradient_loss | -3.04e-08     |\n",
      "|    value_loss           | 3.66e+11      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 2.26e+03      |\n",
      "|    ep_rew_mean          | 1.14e+08      |\n",
      "| time/                   |               |\n",
      "|    fps                  | 765           |\n",
      "|    iterations           | 22            |\n",
      "|    time_elapsed         | 29            |\n",
      "|    total_timesteps      | 22528         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.2456439e-08 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.1          |\n",
      "|    explained_variance   | 0             |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 8.84e+10      |\n",
      "|    n_updates            | 210           |\n",
      "|    policy_gradient_loss | -5.04e-07     |\n",
      "|    value_loss           | 1.29e+11      |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 2.26e+03     |\n",
      "|    ep_rew_mean          | 1.14e+08     |\n",
      "| time/                   |              |\n",
      "|    fps                  | 777          |\n",
      "|    iterations           | 23           |\n",
      "|    time_elapsed         | 30           |\n",
      "|    total_timesteps      | 23552        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 9.720679e-09 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.1         |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 3.52e+11     |\n",
      "|    n_updates            | 220          |\n",
      "|    policy_gradient_loss | -1.25e-07    |\n",
      "|    value_loss           | 5.36e+11     |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 2.26e+03      |\n",
      "|    ep_rew_mean          | 1.14e+08      |\n",
      "| time/                   |               |\n",
      "|    fps                  | 789           |\n",
      "|    iterations           | 24            |\n",
      "|    time_elapsed         | 31            |\n",
      "|    total_timesteps      | 24576         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.0652002e-08 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.1          |\n",
      "|    explained_variance   | -1.19e-07     |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 8.81e+11      |\n",
      "|    n_updates            | 230           |\n",
      "|    policy_gradient_loss | -1.66e-07     |\n",
      "|    value_loss           | 1.62e+12      |\n",
      "-------------------------------------------\n",
      "[EpisodeLogger] steps=4828 return=314770624.00 worked=0 speculated=0 rested=0 age=0.00 wealth=0.00 total_steps=25187\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 2.52e+03      |\n",
      "|    ep_rew_mean          | 1.34e+08      |\n",
      "| time/                   |               |\n",
      "|    fps                  | 800           |\n",
      "|    iterations           | 25            |\n",
      "|    time_elapsed         | 31            |\n",
      "|    total_timesteps      | 25600         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 6.1118044e-09 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.1          |\n",
      "|    explained_variance   | -2.38e-07     |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 2.37e+12      |\n",
      "|    n_updates            | 240           |\n",
      "|    policy_gradient_loss | -4.92e-07     |\n",
      "|    value_loss           | 4.13e+12      |\n",
      "-------------------------------------------\n",
      "[EpisodeLogger] steps=1287 return=17680550.00 worked=0 speculated=0 rested=0 age=0.00 wealth=0.00 total_steps=26474\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 2.41e+03     |\n",
      "|    ep_rew_mean          | 1.24e+08     |\n",
      "| time/                   |              |\n",
      "|    fps                  | 784          |\n",
      "|    iterations           | 26           |\n",
      "|    time_elapsed         | 33           |\n",
      "|    total_timesteps      | 26624        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 7.683411e-09 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.1         |\n",
      "|    explained_variance   | 1.79e-07     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 3.82e+12     |\n",
      "|    n_updates            | 250          |\n",
      "|    policy_gradient_loss | -1.01e-06    |\n",
      "|    value_loss           | 4.34e+12     |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 2.41e+03      |\n",
      "|    ep_rew_mean          | 1.24e+08      |\n",
      "| time/                   |               |\n",
      "|    fps                  | 772           |\n",
      "|    iterations           | 27            |\n",
      "|    time_elapsed         | 35            |\n",
      "|    total_timesteps      | 27648         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.4202669e-08 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.1          |\n",
      "|    explained_variance   | 2e-05         |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 4.78e+10      |\n",
      "|    n_updates            | 260           |\n",
      "|    policy_gradient_loss | -3.54e-06     |\n",
      "|    value_loss           | 1.05e+11      |\n",
      "-------------------------------------------\n",
      "[EpisodeLogger] steps=1817 return=29233356.00 worked=0 speculated=0 rested=0 age=0.00 wealth=0.00 total_steps=28291\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 2.36e+03     |\n",
      "|    ep_rew_mean          | 1.16e+08     |\n",
      "| time/                   |              |\n",
      "|    fps                  | 782          |\n",
      "|    iterations           | 28           |\n",
      "|    time_elapsed         | 36           |\n",
      "|    total_timesteps      | 28672        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 7.043127e-09 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.1         |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.82e+10     |\n",
      "|    n_updates            | 270          |\n",
      "|    policy_gradient_loss | -2.74e-08    |\n",
      "|    value_loss           | 7.33e+10     |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 2.36e+03      |\n",
      "|    ep_rew_mean          | 1.16e+08      |\n",
      "| time/                   |               |\n",
      "|    fps                  | 770           |\n",
      "|    iterations           | 29            |\n",
      "|    time_elapsed         | 38            |\n",
      "|    total_timesteps      | 29696         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.2340024e-08 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.1          |\n",
      "|    explained_variance   | 9.72e-06      |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 6.38e+10      |\n",
      "|    n_updates            | 280           |\n",
      "|    policy_gradient_loss | -5.51e-07     |\n",
      "|    value_loss           | 1.53e+11      |\n",
      "-------------------------------------------\n",
      "[EpisodeLogger] steps=2360 return=94251256.00 worked=0 speculated=0 rested=0 age=0.00 wealth=0.00 total_steps=30651\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 2.36e+03    |\n",
      "|    ep_rew_mean          | 1.14e+08    |\n",
      "| time/                   |             |\n",
      "|    fps                  | 779         |\n",
      "|    iterations           | 30          |\n",
      "|    time_elapsed         | 39          |\n",
      "|    total_timesteps      | 30720       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 9.95351e-09 |\n",
      "|    clip_fraction        | 0           |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.1        |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 3.96e+11    |\n",
      "|    n_updates            | 290         |\n",
      "|    policy_gradient_loss | -3.85e-07   |\n",
      "|    value_loss           | 4.84e+11    |\n",
      "-----------------------------------------\n",
      "[EpisodeLogger] steps=1072 return=9865487.00 worked=0 speculated=0 rested=0 age=0.00 wealth=0.00 total_steps=31723\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 2.27e+03     |\n",
      "|    ep_rew_mean          | 1.07e+08     |\n",
      "| time/                   |              |\n",
      "|    fps                  | 771          |\n",
      "|    iterations           | 31           |\n",
      "|    time_elapsed         | 41           |\n",
      "|    total_timesteps      | 31744        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 4.773028e-09 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.1         |\n",
      "|    explained_variance   | 1.24e-05     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 6.85e+11     |\n",
      "|    n_updates            | 300          |\n",
      "|    policy_gradient_loss | -5.37e-07    |\n",
      "|    value_loss           | 1.24e+12     |\n",
      "------------------------------------------\n",
      "[EpisodeLogger] steps=471 return=187536.64 worked=0 speculated=0 rested=0 age=0.00 wealth=0.00 total_steps=32194\n",
      "[EpisodeLogger] steps=438 return=2076103.88 worked=0 speculated=0 rested=0 age=0.00 wealth=0.00 total_steps=32632\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 2.04e+03     |\n",
      "|    ep_rew_mean          | 9.35e+07     |\n",
      "| time/                   |              |\n",
      "|    fps                  | 759          |\n",
      "|    iterations           | 32           |\n",
      "|    time_elapsed         | 43           |\n",
      "|    total_timesteps      | 32768        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 5.355105e-09 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.1         |\n",
      "|    explained_variance   | 4.77e-06     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.6e+10      |\n",
      "|    n_updates            | 310          |\n",
      "|    policy_gradient_loss | -6.46e-07    |\n",
      "|    value_loss           | 3.9e+10      |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 2.04e+03     |\n",
      "|    ep_rew_mean          | 9.35e+07     |\n",
      "| time/                   |              |\n",
      "|    fps                  | 735          |\n",
      "|    iterations           | 33           |\n",
      "|    time_elapsed         | 45           |\n",
      "|    total_timesteps      | 33792        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 9.778887e-09 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.1         |\n",
      "|    explained_variance   | 6.99e-05     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 5.98e+09     |\n",
      "|    n_updates            | 320          |\n",
      "|    policy_gradient_loss | -5.73e-07    |\n",
      "|    value_loss           | 5.94e+09     |\n",
      "------------------------------------------\n",
      "[EpisodeLogger] steps=1479 return=18873636.00 worked=0 speculated=0 rested=0 age=0.00 wealth=0.00 total_steps=34111\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 2.01e+03      |\n",
      "|    ep_rew_mean          | 8.91e+07      |\n",
      "| time/                   |               |\n",
      "|    fps                  | 743           |\n",
      "|    iterations           | 34            |\n",
      "|    time_elapsed         | 46            |\n",
      "|    total_timesteps      | 34816         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 2.8172508e-08 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.1          |\n",
      "|    explained_variance   | 2.38e-07      |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 3.01e+10      |\n",
      "|    n_updates            | 330           |\n",
      "|    policy_gradient_loss | -7.81e-06     |\n",
      "|    value_loss           | 4.66e+10      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 2.01e+03      |\n",
      "|    ep_rew_mean          | 8.91e+07      |\n",
      "| time/                   |               |\n",
      "|    fps                  | 735           |\n",
      "|    iterations           | 35            |\n",
      "|    time_elapsed         | 48            |\n",
      "|    total_timesteps      | 35840         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.1001248e-08 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.1          |\n",
      "|    explained_variance   | 1.03e-05      |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 3.64e+10      |\n",
      "|    n_updates            | 340           |\n",
      "|    policy_gradient_loss | -2.09e-06     |\n",
      "|    value_loss           | 7.12e+10      |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 2.01e+03     |\n",
      "|    ep_rew_mean          | 8.91e+07     |\n",
      "| time/                   |              |\n",
      "|    fps                  | 743          |\n",
      "|    iterations           | 36           |\n",
      "|    time_elapsed         | 49           |\n",
      "|    total_timesteps      | 36864        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 6.344635e-09 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.1         |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 6.26e+10     |\n",
      "|    n_updates            | 350          |\n",
      "|    policy_gradient_loss | -1.25e-07    |\n",
      "|    value_loss           | 2.37e+11     |\n",
      "------------------------------------------\n",
      "[EpisodeLogger] steps=3547 return=139913392.00 worked=0 speculated=0 rested=0 age=0.00 wealth=0.00 total_steps=37658\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 2.09e+03     |\n",
      "|    ep_rew_mean          | 9.2e+07      |\n",
      "| time/                   |              |\n",
      "|    fps                  | 750          |\n",
      "|    iterations           | 37           |\n",
      "|    time_elapsed         | 50           |\n",
      "|    total_timesteps      | 37888        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 2.561137e-09 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.1         |\n",
      "|    explained_variance   | -3.58e-07    |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 6.31e+11     |\n",
      "|    n_updates            | 360          |\n",
      "|    policy_gradient_loss | -1.74e-07    |\n",
      "|    value_loss           | 9.71e+11     |\n",
      "------------------------------------------\n",
      "[EpisodeLogger] steps=695 return=7149519.00 worked=0 speculated=0 rested=0 age=0.00 wealth=0.00 total_steps=38353\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 2.02e+03     |\n",
      "|    ep_rew_mean          | 8.75e+07     |\n",
      "| time/                   |              |\n",
      "|    fps                  | 743          |\n",
      "|    iterations           | 38           |\n",
      "|    time_elapsed         | 52           |\n",
      "|    total_timesteps      | 38912        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 4.831236e-09 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.1         |\n",
      "|    explained_variance   | 5.96e-08     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.3e+12      |\n",
      "|    n_updates            | 370          |\n",
      "|    policy_gradient_loss | -8.38e-10    |\n",
      "|    value_loss           | 1.81e+12     |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 2.02e+03      |\n",
      "|    ep_rew_mean          | 8.75e+07      |\n",
      "| time/                   |               |\n",
      "|    fps                  | 735           |\n",
      "|    iterations           | 39            |\n",
      "|    time_elapsed         | 54            |\n",
      "|    total_timesteps      | 39936         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.0186341e-08 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.1          |\n",
      "|    explained_variance   | 0.000114      |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 1.45e+10      |\n",
      "|    n_updates            | 380           |\n",
      "|    policy_gradient_loss | -5.61e-07     |\n",
      "|    value_loss           | 3.91e+10      |\n",
      "-------------------------------------------\n",
      "[EpisodeLogger] steps=2472 return=72768328.00 worked=0 speculated=0 rested=0 age=0.00 wealth=0.00 total_steps=40825\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 2.04e+03     |\n",
      "|    ep_rew_mean          | 8.68e+07     |\n",
      "| time/                   |              |\n",
      "|    fps                  | 742          |\n",
      "|    iterations           | 40           |\n",
      "|    time_elapsed         | 55           |\n",
      "|    total_timesteps      | 40960        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 9.778887e-09 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.1         |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.2e+11      |\n",
      "|    n_updates            | 390          |\n",
      "|    policy_gradient_loss | -5.45e-08    |\n",
      "|    value_loss           | 2.31e+11     |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 2.04e+03      |\n",
      "|    ep_rew_mean          | 8.68e+07      |\n",
      "| time/                   |               |\n",
      "|    fps                  | 735           |\n",
      "|    iterations           | 41            |\n",
      "|    time_elapsed         | 57            |\n",
      "|    total_timesteps      | 41984         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 5.7043508e-09 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.1          |\n",
      "|    explained_variance   | 4.83e-05      |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 7.2e+11       |\n",
      "|    n_updates            | 400           |\n",
      "|    policy_gradient_loss | -4.87e-07     |\n",
      "|    value_loss           | 9.08e+11      |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 2.04e+03     |\n",
      "|    ep_rew_mean          | 8.68e+07     |\n",
      "| time/                   |              |\n",
      "|    fps                  | 742          |\n",
      "|    iterations           | 42           |\n",
      "|    time_elapsed         | 57           |\n",
      "|    total_timesteps      | 43008        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 5.995389e-09 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.1         |\n",
      "|    explained_variance   | 3.25e-05     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 5.79e+10     |\n",
      "|    n_updates            | 410          |\n",
      "|    policy_gradient_loss | -1.42e-07    |\n",
      "|    value_loss           | 8.37e+10     |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 2.04e+03      |\n",
      "|    ep_rew_mean          | 8.68e+07      |\n",
      "| time/                   |               |\n",
      "|    fps                  | 748           |\n",
      "|    iterations           | 43            |\n",
      "|    time_elapsed         | 58            |\n",
      "|    total_timesteps      | 44032         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 3.1432137e-09 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.1          |\n",
      "|    explained_variance   | 0             |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 3.91e+11      |\n",
      "|    n_updates            | 420           |\n",
      "|    policy_gradient_loss | 8.5e-08       |\n",
      "|    value_loss           | 7.07e+11      |\n",
      "-------------------------------------------\n",
      "训练完成 ✅\n",
      "一共记录了 20 个 episode\n",
      "             r     l      t  days_worked  days_speculated  days_rested  \\\n",
      "0  611980160.0  6523   6523            0                0            0   \n",
      "1    2487373.5   463   6986            0                0            0   \n",
      "2    7864898.5   828   7814            0                0            0   \n",
      "3  130537424.0  3056  10870            0                0            0   \n",
      "4  179679008.0  3702  14572            0                0            0   \n",
      "\n",
      "   final_age  final_wealth  \n",
      "0          0             0  \n",
      "1          0             0  \n",
      "2          0             0  \n",
      "3          0             0  \n",
      "4          0             0  \n",
      "已保存到 CSV 文件：episode_logs.csv\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gymnasium as gym\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, VecMonitor\n",
    "from sb3_contrib import RecurrentPPO\n",
    "\n",
    "# === 1) 创建环境（按天推进；max_age 用天数表示：80 年 = 80*365 天） ===\n",
    "def make_env():\n",
    "    # 这里请确保你上一格的环境类名和参数一致（按天计息/计损耗）\n",
    "    return SpeculationVsLaborEnv(\n",
    "        max_age=80*365,          # 80年寿命上限（按天）\n",
    "        labor_income=100,        # 每天劳动的固定收益（可按需改）\n",
    "        labor_health_cost=2,     # 每天劳动健康消耗\n",
    "        p_win=0.4,               # 投机获胜概率\n",
    "        speculative_win_return=500,  # 投机成功时收益（每天一次决策）\n",
    "        speculative_loss=400,        # 投机失败时损失\n",
    "        invest_health_cost=1,    # 投机健康消耗\n",
    "        rest_health_gain=3,      # 休息每天恢复的健康值\n",
    "        r_save=0.0001,           # 存款日利率\n",
    "        r_debt=0.0005            # 债务日利率\n",
    "    )\n",
    "\n",
    "# VecEnv + 监控（RecurrentPPO 需要 VecEnv；Monitor 能记录回合统计信息）\n",
    "env = DummyVecEnv([make_env])\n",
    "env = VecMonitor(env, filename=None)\n",
    "\n",
    "# === 2) 定义并创建 RecurrentPPO（LSTM 策略） ===\n",
    "# 设备建议用CPU（Mlp/LSTM的小模型在CPU上更高效）\n",
    "policy_kwargs = dict(\n",
    "    lstm_hidden_size=128,   # LSTM隐藏层（可调小到64以更省资源）\n",
    "    n_lstm_layers=1,        # LSTM层数\n",
    "    shared_lstm=False       # Actor/Critic是否共享LSTM\n",
    ")\n",
    "\n",
    "model = RecurrentPPO(\n",
    "    policy=\"MlpLstmPolicy\",\n",
    "    env=env,\n",
    "    verbose=1,\n",
    "    device=\"cuda\",           # GF63 建议先用 CPU；如果你想用GPU可改 \"cuda\"\n",
    "    n_steps=1024,           # rollout 长度（越大越稳定，但显存/内存占用也更大）\n",
    "    batch_size=256,         # 训练batch\n",
    "    learning_rate=3e-4,\n",
    "    gamma=0.999,            # 每日折扣，贴近“长期回报”\n",
    "    gae_lambda=0.95,\n",
    "    ent_coef=0.0,\n",
    "    clip_range=0.2,\n",
    "    policy_kwargs=policy_kwargs,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "# === 3) 训练 ===\n",
    "years_to_simulate = 40\n",
    "steps_per_episode = years_to_simulate * 365\n",
    "episodes = 3\n",
    "total_timesteps = steps_per_episode * episodes\n",
    "\n",
    "print(f\"开始训练，总步数：{total_timesteps}\")\n",
    "\n",
    "episode_logger = EpisodeLoggerCallback(verbose=1)\n",
    "\n",
    "model.learn(\n",
    "    total_timesteps=total_timesteps,\n",
    "    callback=episode_logger\n",
    ")\n",
    "\n",
    "print(\"训练完成 ✅\")\n",
    "print(\"一共记录了\", len(episode_logger.episode_logs), \"个 episode\")\n",
    "print(pd.DataFrame(episode_logger.episode_logs).head())\n",
    "\n",
    "# 假设你已经有了 DataFrame\n",
    "df = pd.DataFrame(episode_logger.episode_logs)\n",
    "\n",
    "# 保存到 CSV 文件\n",
    "csv_path = \"episode_logs.csv\"  # 你可以改成任意路径或文件名\n",
    "df.to_csv(csv_path, index=False)  # index=False 避免保存行号\n",
    "\n",
    "print(f\"已保存到 CSV 文件：{csv_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1cc963c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "已保存到 svl_lstm_eval.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Episode</th>\n",
       "      <th>Final_Wealth</th>\n",
       "      <th>Final_Health</th>\n",
       "      <th>Final_Age_years</th>\n",
       "      <th>Debt</th>\n",
       "      <th>Savings</th>\n",
       "      <th>Total_Reward</th>\n",
       "      <th>Work_Count</th>\n",
       "      <th>Invest_Count</th>\n",
       "      <th>Rest_Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>5012.770996</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.136986</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5012.770996</td>\n",
       "      <td>127721.270985</td>\n",
       "      <td>50</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>5012.770996</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.136986</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5012.770996</td>\n",
       "      <td>127721.270985</td>\n",
       "      <td>50</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>5012.770996</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.136986</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5012.770996</td>\n",
       "      <td>127721.270985</td>\n",
       "      <td>50</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>5012.770996</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.136986</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5012.770996</td>\n",
       "      <td>127721.270985</td>\n",
       "      <td>50</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>5012.770996</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.136986</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5012.770996</td>\n",
       "      <td>127721.270985</td>\n",
       "      <td>50</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Episode  Final_Wealth  Final_Health  Final_Age_years  Debt      Savings  \\\n",
       "0        0   5012.770996           0.0         0.136986   0.0  5012.770996   \n",
       "1        1   5012.770996           0.0         0.136986   0.0  5012.770996   \n",
       "2        2   5012.770996           0.0         0.136986   0.0  5012.770996   \n",
       "3        3   5012.770996           0.0         0.136986   0.0  5012.770996   \n",
       "4        4   5012.770996           0.0         0.136986   0.0  5012.770996   \n",
       "\n",
       "    Total_Reward  Work_Count  Invest_Count  Rest_Count  \n",
       "0  127721.270985          50             0           0  \n",
       "1  127721.270985          50             0           0  \n",
       "2  127721.270985          50             0           0  \n",
       "3  127721.270985          50             0           0  \n",
       "4  127721.270985          50             0           0  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# === 4) 测试 & 保存结果（带 LSTM 状态） ===\n",
    "results = []\n",
    "test_env = make_env()\n",
    "\n",
    "num_eval_episodes = 200  # 评估 200 次人生\n",
    "\n",
    "for ep in range(num_eval_episodes):\n",
    "    obs, _ = test_env.reset()\n",
    "    done = False\n",
    "    total_reward = 0.0\n",
    "\n",
    "    # RecurrentPPO 需要跟踪 LSTM state 与 episode_start\n",
    "    lstm_state = None\n",
    "    episode_start = True\n",
    "\n",
    "    work_count = 0\n",
    "    invest_count = 0\n",
    "    rest_count = 0\n",
    "\n",
    "    while not done:\n",
    "        action, lstm_state = model.predict(\n",
    "            obs,\n",
    "            state=lstm_state,\n",
    "            episode_start=np.array([episode_start]),\n",
    "            deterministic=True\n",
    "        )\n",
    "        episode_start = False  # 只有 reset 后的第一步是 True\n",
    "\n",
    "        if action == 0:\n",
    "            work_count += 1\n",
    "        elif action == 1:\n",
    "            invest_count += 1\n",
    "        else:\n",
    "            rest_count += 1\n",
    "\n",
    "        obs, reward, done, truncated, info = test_env.step(action)\n",
    "        total_reward += float(reward)\n",
    "\n",
    "        # Gymnasium 风格：terminated 或 truncated 任一为真都算结束\n",
    "        done = bool(done or truncated)\n",
    "\n",
    "    wealth, health, age, debt, savings = obs.astype(float)\n",
    "\n",
    "    results.append({\n",
    "        \"Episode\": ep,\n",
    "        \"Final_Wealth\": wealth,\n",
    "        \"Final_Health\": health,\n",
    "        \"Final_Age_years\": age / 365.0,  # 年龄换成年\n",
    "        \"Debt\": debt,\n",
    "        \"Savings\": savings,\n",
    "        \"Total_Reward\": total_reward,\n",
    "        \"Work_Count\": work_count,\n",
    "        \"Invest_Count\": invest_count,\n",
    "        \"Rest_Count\": rest_count\n",
    "    })\n",
    "\n",
    "df = pd.DataFrame(results)\n",
    "df.to_csv(\"svl_lstm_eval.csv\", index=False)\n",
    "print(\"已保存到 svl_lstm_eval.csv\")\n",
    "df.head()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "testing",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
